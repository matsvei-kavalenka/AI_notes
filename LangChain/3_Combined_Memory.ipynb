{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-05T14:56:47.167844Z",
     "start_time": "2025-06-05T14:56:47.148434Z"
    }
   },
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:56:56.116354Z",
     "start_time": "2025-06-05T14:56:53.978657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.memory import (ConversationBufferMemory, \n",
    "                              ConversationSummaryMemory, \n",
    "                              CombinedMemory)\n",
    "from langchain.globals import set_verbose\n",
    "\n",
    "set_verbose(True)"
   ],
   "id": "339c0980fb093b23",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:57:14.957048Z",
     "start_time": "2025-06-05T14:57:14.917047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chat = ChatOpenAI(model_name = 'gpt-4o-mini', \n",
    "                  seed=365,\n",
    "                  temperature = 0,\n",
    "                  max_tokens = 100)"
   ],
   "id": "b657a1f89f2bd5d9",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:57:23.523912Z",
     "start_time": "2025-06-05T14:57:23.503913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TEMPLATE = '''\n",
    "The following is a friendly conversation between a human and an AI. \n",
    "The AI is talkative and provides lots of specific details from its context. \n",
    "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Past messages:\n",
    "{message_buffer_log}\n",
    "\n",
    "Conversation summary:\n",
    "{message_summary_log}\n",
    "\n",
    "Human: {question}\n",
    "AI:\n",
    "'''\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template = TEMPLATE)"
   ],
   "id": "98d8048df0c72c31",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:57:31.326412Z",
     "start_time": "2025-06-05T14:57:31.274417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chat_buffer_memory = ConversationBufferMemory(memory_key = 'message_buffer_log', \n",
    "                                              input_key = 'question',\n",
    "                                              return_messages = False)\n",
    "\n",
    "chat_summary_memory = ConversationSummaryMemory(llm = ChatOpenAI(), \n",
    "                                                memory_key = 'message_summary_log', \n",
    "                                                input_key = 'question',\n",
    "                                                return_messages = False)\n",
    "\n",
    "chat_combined_memory = CombinedMemory(memories = [chat_buffer_memory, \n",
    "                                                  chat_summary_memory])"
   ],
   "id": "43e46981982dc0ac",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matsv\\AppData\\Local\\Temp\\ipykernel_2896\\1570050116.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  chat_buffer_memory = ConversationBufferMemory(memory_key = 'message_buffer_log',\n",
      "C:\\Users\\matsv\\AppData\\Local\\Temp\\ipykernel_2896\\1570050116.py:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  chat_summary_memory = ConversationSummaryMemory(llm = ChatOpenAI(),\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:57:38.241138Z",
     "start_time": "2025-06-05T14:57:38.221135Z"
    }
   },
   "cell_type": "code",
   "source": "print(chat_combined_memory.load_memory_variables({})['message_summary_log'])",
   "id": "2660149e4fb2a372",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:57:42.765514Z",
     "start_time": "2025-06-05T14:57:42.747523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chain = LLMChain(llm = chat, \n",
    "                 prompt = prompt_template, \n",
    "                 memory = chat_combined_memory)"
   ],
   "id": "980f60f194c1daf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matsv\\AppData\\Local\\Temp\\ipykernel_2896\\3494076389.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm = chat,\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:57:51.756314Z",
     "start_time": "2025-06-05T14:57:47.113602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chain.invoke({'question':'''Can you give me an interesting fact I probably didn't know about?'''})\n",
    "# chain.invoke({'question':'''Can you elaborate a bit more on this fact?'''})"
   ],
   "id": "bdeb03fddf9c886d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3m\n",
      "The following is a friendly conversation between a human and an AI. \n",
      "The AI is talkative and provides lots of specific details from its context. \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Past messages:\n",
      "\n",
      "\n",
      "Conversation summary:\n",
      "\n",
      "\n",
      "Human: Can you give me an interesting fact I probably didn't know about?\n",
      "AI:\n",
      "\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mProgressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
      "\n",
      "EXAMPLE\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Why do you think artificial intelligence is a force for good?\n",
      "AI: Because artificial intelligence will help humans reach their full potential.\n",
      "\n",
      "New summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
      "END OF EXAMPLE\n",
      "\n",
      "Current summary:\n",
      "\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Can you give me an interesting fact I probably didn't know about?\n",
      "AI: Sure! Did you know that honey never spoils? Archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible! Honey's long shelf life is due to its low moisture content and acidic pH, which create an inhospitable environment for bacteria and microorganisms. So, if you have a jar of honey sitting in your pantry, it could last indefinitely!\n",
      "\n",
      "New summary:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': \"Can you give me an interesting fact I probably didn't know about?\",\n",
       " 'message_buffer_log': '',\n",
       " 'message_summary_log': '',\n",
       " 'text': \"Sure! Did you know that honey never spoils? Archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible! Honey's long shelf life is due to its low moisture content and acidic pH, which create an inhospitable environment for bacteria and microorganisms. So, if you have a jar of honey sitting in your pantry, it could last indefinitely!\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7009c9ab48e0e169"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
